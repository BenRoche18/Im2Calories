{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1aacf22a6e5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow.keras.utils as utils\n",
    "\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from usda import UsdaClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declare parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "\n",
    "IMAGE_SIZE = 299\n",
    "\n",
    "API_KEY = \"4INghUtThsIBWPTIcvfKyf0kNS6MtSXcC4R6mpNB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enable GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch model architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fetching model... \", end=\"\")\n",
    "# fetch feature extractor\n",
    "model = torchvision.models.inception_v3(pretrained=False)\n",
    "\n",
    "# handle auxilary net\n",
    "num_features = model.AuxLogits.fc.in_features\n",
    "model.AuxLogits.fc = torch.nn.Linear(num_features, 101)\n",
    "\n",
    "# handle primary net\n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_features, 101)\n",
    "\n",
    "model.to(device)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load model weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(ROOT_DIR, \"models\", \"food_classification\", \"fine_tuning.pt\")\n",
    "\n",
    "print(\"Loading learnt model weights...\", end=\"\")\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch test images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOOD101_DIR = os.path.join(os.path.abspath(os.sep), \"Datasets\", \"food101\")\n",
    "TEST_DIR = os.path.join(FOOD101_DIR, \"test\")\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "  transforms.Resize(IMAGE_SIZE),\n",
    "  transforms.CenterCrop(IMAGE_SIZE),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_raw = torchvision.datasets.ImageFolder(TEST_DIR, transform=test_transform)\n",
    "test_size = len(test_raw)\n",
    "\n",
    "class_names = test_raw.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run test images on model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(images):\n",
    "    images = images.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "    \n",
    "    return [class_names[i] for i in predictions]\n",
    "\n",
    "# randomly select 20 images from validation set\n",
    "images = []\n",
    "real_classes = []\n",
    "for i in range(20):\n",
    "    j = np.random.randint(test_size)\n",
    "    img, lbl = test_raw[j]\n",
    "    images.append(img.numpy())\n",
    "    real_classes.append(class_names[lbl])\n",
    "\n",
    "inputs = torch.tensor(images)\n",
    "predicted_classes = predict(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output predictions against real**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols, rows = 5, 4\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "fig.suptitle(\"Results from test predictions\")\n",
    "\n",
    "for i in range(1, cols*rows+1):\n",
    "  fig.add_subplot(rows, cols, i)\n",
    "\n",
    "  # randomly sellect image from dataset\n",
    "  img = images[i-1]\n",
    "  img = np.transpose(img, (1,2,0))\n",
    "  img = img * (0.485, 0.456, 0.406) + (0.229, 0.224, 0.225)\n",
    "\n",
    "  # display title as green if correct otherwise red\n",
    "  color = \"green\" if predicted_classes[i-1] == real_classes[i-1] else \"red\"\n",
    "  plt.title(predicted_classes[i-1], color=color)\n",
    "  plt.axis(\"off\")\n",
    "  img = np.clip(img, 0, 1)\n",
    "  plt.imshow(img, interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load image from url**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(url):\n",
    "    filename = url.split('/')[-1]\n",
    "    img = utils.get_file(filename, url)\n",
    "    img = Image.open(img)\n",
    "    return img\n",
    "\n",
    "img = loadImage(\"https://img.buzzfeed.com/video-api-prod/assets/1b97c3bbb24c45b892f582c8286c54bf/Thumb_A_FB.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format image for model input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "img = transform(img)\n",
    "images = img.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply model on image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_class = predict(images)[0]\n",
    "img_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nutritional lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search USDA for foods with predicted class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = UsdaClient(API_KEY)\n",
    "\n",
    "# seach USDA with predicted image class as search term\n",
    "food_results = client.search_foods(img_class, 1)\n",
    "\n",
    "food = next(food_results)\n",
    "\n",
    "# fetch food report for top result\n",
    "food_report = client.get_food_report(food.id)\n",
    "for nutrient in food_report.nutrients:\n",
    "    print(nutrient.name, nutrient.value, nutrient.unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

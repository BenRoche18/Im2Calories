{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/BenRoche18/Im2Calories/blob/master/food_classification_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_nLyDv8Vm2C"
   },
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ISJ0ihdNdjhS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import copy\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm.autonotebook import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import utility functions from Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"utilities\"))\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wjsABqdVyKa"
   },
   "source": [
    "**Enable GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7rQNqPXEV4Ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8eHVTWDif9d"
   },
   "source": [
    "**Declare parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Y0qqicdirJ_"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUlZthLcWNJT"
   },
   "source": [
    "# Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch condensed classes from text file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOOD256_DIR = os.path.join(os.path.abspath(os.sep), \"Datasets\", \"food256\")\n",
    "CONDENSED_CLASSES_PATH = os.path.join(FOOD256_DIR, \"condensed-category.txt\")\n",
    "\n",
    "with open(CONDENSED_CLASSES_PATH, 'r') as file:\n",
    "    file.readline()\n",
    "    classes = {line.split('\\t')[0]:line.split('\\t')[1].strip() for line in file.readlines()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract condensed classes from dataset**\n",
    "\n",
    "Extract images and labels that correspond to condensed dataset and relabel classes so they are incremented by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Datasets\\\\food256\\\\2\\\\301.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9d6fc07f96d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;31m# copy img to images directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mdest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMAGES_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;31m# normalise class_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Datasets\\\\food256\\\\2\\\\301.jpg'"
     ]
    }
   ],
   "source": [
    "IMAGES_DIR = os.path.join(FOOD256_DIR, \"condensed images\")\n",
    "LABELS_DIR = os.path.join(FOOD256_DIR, \"condensed labels\")\n",
    "\n",
    "if not os.path.exists(IMAGES_DIR):\n",
    "    os.makedirs(IMAGES_DIR)\n",
    "    os.makedirs(LABELS_DIR)\n",
    "\n",
    "    for class_id in classes.keys():\n",
    "        class_path = os.path.join(FOOD256_DIR, class_id, \"bb_info.txt\")\n",
    "        with open(class_path, 'r') as file:\n",
    "            # ignore headers\n",
    "            file.readline()\n",
    "            for line in file.readlines():\n",
    "                line = line.strip().split()\n",
    "                img_path = os.path.join(FOOD256_DIR, class_id, line[0]+\".jpg\")\n",
    "\n",
    "                # copy img to images directory\n",
    "                dest = os.path.join(IMAGES_DIR, line[0]+\".jpg\")\n",
    "                try:\n",
    "                    shutil.copy(img_path, dest)\n",
    "\n",
    "                    # normalise class_id\n",
    "                    class_id = str(list(classes.keys()).index(class_id) + 1)\n",
    "\n",
    "                    # copy bounding box annotation into txt file in labels directory\n",
    "                    with open(os.path.join(LABELS_DIR, line[0]+\".txt\"), 'w') as box_file:\n",
    "                        box = class_id + \" \" + \" \".join(line[1:])\n",
    "                        box_file.write(box)\n",
    "                except FileNotFoundError:\n",
    "                    print(dest, \"could not be found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define FoodDataset class as subclass of Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.images = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        self.classes = []\n",
    "        with open(os.path.join(root, \"category.txt\"), 'r') as file:\n",
    "            file.readline()\n",
    "            for line in file.readlines():\n",
    "                self.classes.append(line.strip().split(\"\\t\")[1])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # image\n",
    "        img_path = os.path.join(self.root, \"images\", self.images[index])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        #label\n",
    "        lbl_path = img_path.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n",
    "        with open(lbl_path, 'r') as file:\n",
    "            line = file.read().split()\n",
    "            \n",
    "        lbl = int(line[0]) - 1\n",
    "            \n",
    "        x1 = float(line[1])\n",
    "        y1 = float(line[2])\n",
    "        x2 = float(line[3])\n",
    "        y2 = float(line[4])\n",
    "\n",
    "        target = {\n",
    "            \"image_id\": torch.as_tensor([index]),\n",
    "            \"labels\": torch.as_tensor([lbl], dtype=torch.int64),\n",
    "            \"boxes\": torch.as_tensor([[x1, y1, x2, y2]], dtype=torch.float32),\n",
    "            \"area\": torch.as_tensor([(x2-x1)*(y2-y1)]),\n",
    "            \"iscrowd\": torch.as_tensor([0], dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img, target = self.transform(img, target)\n",
    "            \n",
    "        return img, target\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transform for R-CNN input\n",
    "class CustomTransform:\n",
    "    def __init__(self, image_size):\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    def __call__(self, img, target):\n",
    "        # resize to a max of IMAGE_SIZE\n",
    "        w, h = img.size\n",
    "        scale = min(IMAGE_SIZE/w, IMAGE_SIZE/h)\n",
    "        img = transforms.functional.resize(img, (int(h*scale), int(w*scale)))\n",
    "        \n",
    "        # update bounding boxes\n",
    "        target[\"boxes\"] *= scale\n",
    "        \n",
    "        # add padding to a size of IMAGE_SIZE\n",
    "        img = transforms.functional.pad(img, (0, 0, IMAGE_SIZE-int(w*scale), IMAGE_SIZE-int(h*scale)))\n",
    "        \n",
    "        # convert to tensor\n",
    "        img = transforms.functional.to_tensor(img)\n",
    "\n",
    "        # normalize\n",
    "        img = img / 255\n",
    "        return img, target\n",
    "\n",
    "transform = CustomTransform(image_size=IMAGE_SIZE)\n",
    "\n",
    "food_raw = FoodDataset(FOOD256_DIR, transform=transform)\n",
    "food_size = len(food_raw)\n",
    "\n",
    "class_names = food_raw.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split dataset into train and val sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randperm(food_size).tolist()\n",
    "\n",
    "# define how to batch data (i.e not combine target dicitonaries)\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# take 80% as training\n",
    "train_raw = torch.utils.data.Subset(food_raw, indices[:int(0.8*food_size)])\n",
    "train_size = len(train_raw)\n",
    "train_loader = torch.utils.data.DataLoader(train_raw, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# take 20% as validation\n",
    "val_raw = torch.utils.data.Subset(food_raw, indices[int(0.8*food_size):])\n",
    "val_size = len(val_raw)\n",
    "val_loader = torch.utils.data.DataLoader(val_raw, batch_size=max(1, BATCH_SIZE//2), shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show example image with bounding boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXJIlUvAmRJO"
   },
   "outputs": [],
   "source": [
    "fig, axis= plt.subplots(1)\n",
    "\n",
    "#randomly select image from dataset\n",
    "i = np.random.randint(train_size)\n",
    "img = train_raw[i][0].numpy()\n",
    "img = np.transpose(img, (1,2,0))\n",
    "img = img * 255\n",
    "img = np.clip(img, 0, 1)\n",
    "plt.imshow(img)\n",
    "\n",
    "# draw bounding box\n",
    "x1, y1, x2, y2 = train_raw[i][1][\"boxes\"][0]\n",
    "box = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "axis.add_patch(box)\n",
    "\n",
    "plt.title(class_names[train_raw[i][1][\"labels\"][0]])\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xzgyw9d1YtJx"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch pretrained wide resnet on food101 dataset as backbone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE_PATH = os.path.join(ROOT_DIR, \"models\", \"food_classification\", \"fine_tuning_wideres.pt\")\n",
    "\n",
    "print(\"Fetching pretrained backbone... \", end=\"\")\n",
    "backbone = torchvision.models.wide_resnet101_2(pretrained=False)\n",
    "\n",
    "# replace classification head\n",
    "num_features = backbone.fc.in_features\n",
    "backbone.fc = torch.nn.Linear(num_features, 101)\n",
    "\n",
    "# load trained weights\n",
    "backbone.load_state_dict(torch.load(BACKBONE_PATH))\n",
    "\n",
    "# remove classification head\n",
    "backbone = torch.nn.Sequential(*list(backbone.children())[:-2])\n",
    "backbone.out_channels = 2048\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Faster R-CNN from wide resnet backbone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating R-CNN... \", end=\"\")\n",
    "model = torchvision.models.detection.FasterRCNN(backbone, num_classes=len(class_names))\n",
    "\n",
    "model = model.to(device)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvnnS8hWjPSV"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGOLOjPWHxtO"
   },
   "source": [
    "**Declare optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iK1NBW85H1q0"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "# fetch trainable parameters for transfer learning(where requires_grad = true)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sg6d4uuPkWUM"
   },
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(5, 5+num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
    "    evaluate(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "toIULLBt1TBP"
   },
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f407af0g1ZsG"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(ROOT_DIR, \"models\", \"food_detection\", \"wideres_backbone.pt\")\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wHQgtDmp2nK"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMEED9oymhB2SYDFZhAXOf8",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "food_classification_v1.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
